{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "import hashlib\n",
    "\n",
    "import src.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = util.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load train set\n",
    "    x_train = util.pickle_load(params[\"train_feng_set_path\"][0])\n",
    "    y_train = util.pickle_load(params[\"train_feng_set_path\"][1])\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "def load_valid_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load valid set\n",
    "    x_valid = util.pickle_load(params[\"valid_feng_set_path\"][0])\n",
    "    y_valid = util.pickle_load(params[\"valid_feng_set_path\"][1])\n",
    "\n",
    "    return x_valid, y_valid\n",
    "\n",
    "def load_test_feng(params: dict) -> pd.DataFrame:\n",
    "    # Load test set\n",
    "    x_test = util.pickle_load(params[\"test_feng_set_path\"][0])\n",
    "    y_test = util.pickle_load(params[\"test_feng_set_path\"][1])\n",
    "\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(params: dict) -> pd.DataFrame:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Loading dataset.\")\n",
    "\n",
    "    # Load train set\n",
    "    x_train, y_train = load_train_feng(params)\n",
    "\n",
    "    # Load valid set\n",
    "    x_valid, y_valid = load_valid_feng(params)\n",
    "\n",
    "    # Load test set\n",
    "    x_test, y_test = load_test_feng(params)\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Dataset loaded.\")\n",
    "\n",
    "    # Return the dataset\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Log Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_template() -> dict:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Creating training log template.\")\n",
    "    \n",
    "    # Template of training log\n",
    "    logger = {\n",
    "        \"model_name\" : [],\n",
    "        \"model_uid\" : [],\n",
    "        \"training_time\" : [],\n",
    "        \"training_date\" : [],\n",
    "        \"performance\" : [],\n",
    "        \"f1_score_avg\" : [],\n",
    "        \"data_configurations\" : [],\n",
    "    }\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Training log template created.\")\n",
    "\n",
    "    # Return training log template\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_log_updater(current_log: dict, params: dict) -> list:\n",
    "    # Create copy of current log\n",
    "    current_log = copy.deepcopy(current_log)\n",
    "\n",
    "    # Path for training log file\n",
    "    log_path = params[\"training_log_path\"]\n",
    "\n",
    "    # Try to load training log file\n",
    "    try:\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "    # If file not found, create a new one\n",
    "    except FileNotFoundError as fe:\n",
    "        with open(log_path, \"w\") as file:\n",
    "            file.write(\"[]\")\n",
    "        file.close()\n",
    "\n",
    "        with open(log_path, \"r\") as file:\n",
    "            last_log = json.load(file)\n",
    "        file.close()\n",
    "    \n",
    "    # Add current log to previous log\n",
    "    last_log.append(current_log)\n",
    "\n",
    "    # Save updated log\n",
    "    with open(log_path, \"w\") as file:\n",
    "        json.dump(last_log, file)\n",
    "        file.close()\n",
    "\n",
    "    # Return log\n",
    "    return last_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Create Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object(params: dict) -> list:\n",
    "    # Debug message\n",
    "    util.print_debug(\"Creating model objects.\")\n",
    "\n",
    "    # Create model objects\n",
    "    lgr = LogisticRegression()\n",
    "    dct = DecisionTreeClassifier()\n",
    "    rfc = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "    xgb = XGBClassifier()\n",
    "\n",
    "    # Create list of model\n",
    "    list_of_model = [\n",
    "        { \"model_name\": lgr.__class__.__name__, \"model_object\": lgr, \"model_uid\": \"\"},\n",
    "        { \"model_name\": dct.__class__.__name__, \"model_object\": dct, \"model_uid\": \"\"},\n",
    "        { \"model_name\": rfc.__class__.__name__, \"model_object\": rfc, \"model_uid\": \"\"},\n",
    "        { \"model_name\": knn.__class__.__name__, \"model_object\": knn, \"model_uid\": \"\"},\n",
    "        { \"model_name\": xgb.__class__.__name__, \"model_object\": xgb, \"model_uid\": \"\"}\n",
    "    ]\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Model objects created.\")\n",
    "\n",
    "    # Return the list of model\n",
    "    return list_of_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Training Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(configuration_model: str, params: dict, hyperparams_model: list = None):\n",
    "    # Load dataset\n",
    "    x_train, y_train, \\\n",
    "    x_valid, y_valid, \\\n",
    "    x_test, y_test = load_dataset(params)\n",
    "\n",
    "    # Variabel to store trained models\n",
    "    list_of_trained_model = dict()\n",
    "\n",
    "    # Create log template\n",
    "    training_log = training_log_template()\n",
    "\n",
    "    # Training for every data configuration\n",
    "    for config_data in x_train:\n",
    "        # Debug message\n",
    "        util.print_debug(\"Training model based on configuration data: {}\".format(config_data))\n",
    "\n",
    "        # Create model objects\n",
    "        if hyperparams_model == None:\n",
    "            list_of_model = create_model_object(params)\n",
    "        else:\n",
    "            list_of_model = copy.deepcopy(hyperparams_model)\n",
    "\n",
    "        # Variabel to store tained model\n",
    "        trained_model = list()\n",
    "\n",
    "        # Load train data based on its configuration\n",
    "        x_train_data = x_train[config_data]\n",
    "        y_train_data = y_train[config_data]\n",
    "\n",
    "        # Train each model by current dataset configuration\n",
    "        for model in list_of_model:\n",
    "            # Debug message\n",
    "            util.print_debug(\"Training model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Training\n",
    "            training_time = util.time_stamp()\n",
    "            model[\"model_object\"].fit(x_train_data, y_train_data)\n",
    "            training_time = (util.time_stamp() - training_time).total_seconds()\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Evalutaing model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Evaluation\n",
    "            y_predict = model[\"model_object\"].predict(x_valid)\n",
    "            performance = classification_report(y_valid, y_predict, output_dict = True)\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Logging: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Create UID\n",
    "            uid = hashlib.md5(str(training_time).encode()).hexdigest()\n",
    "\n",
    "            # Assign model's UID\n",
    "            model[\"model_uid\"] = uid\n",
    "\n",
    "            # Create training log data\n",
    "            training_log[\"model_name\"].append(\"{}-{}\".format(configuration_model, model[\"model_name\"]))\n",
    "            training_log[\"model_uid\"].append(uid)\n",
    "            training_log[\"training_time\"].append(training_time)\n",
    "            training_log[\"training_date\"].append(util.time_stamp())\n",
    "            training_log[\"performance\"].append(performance)\n",
    "            training_log[\"f1_score_avg\"].append(performance[\"macro avg\"][\"f1-score\"])\n",
    "            training_log[\"data_configurations\"].append(config_data)\n",
    "\n",
    "            # Collect current trained model\n",
    "            trained_model.append(copy.deepcopy(model))\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Model {} has been trained for configuration data {}.\".format(model[\"model_name\"], config_data))\n",
    "        \n",
    "        # Collect current trained list of model\n",
    "        list_of_trained_model[config_data] = copy.deepcopy(trained_model)\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"All combination models and configuration data has been trained.\")\n",
    "    \n",
    "    # Return list trained model\n",
    "    return list_of_trained_model, training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:41:17.820462 Loading dataset.\n",
      "2022-12-04 18:41:17.943131 Dataset loaded.\n",
      "2022-12-04 18:41:17.943329 Creating training log template.\n",
      "2022-12-04 18:41:17.943420 Training log template created.\n",
      "2022-12-04 18:41:17.943517 Training model based on configuration data: Undersampling\n",
      "2022-12-04 18:41:17.943599 Creating model objects.\n",
      "2022-12-04 18:41:17.943787 Model objects created.\n",
      "2022-12-04 18:41:17.943841 Training model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:41:18.937424 Evalutaing model: LogisticRegression\n",
      "2022-12-04 18:41:18.983581 Logging: LogisticRegression\n",
      "2022-12-04 18:41:18.984141 Model LogisticRegression has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:41:18.985155 Training model: DecisionTreeClassifier\n",
      "2022-12-04 18:41:19.157901 Evalutaing model: DecisionTreeClassifier\n",
      "2022-12-04 18:41:19.210609 Logging: DecisionTreeClassifier\n",
      "2022-12-04 18:41:19.212546 Model DecisionTreeClassifier has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:41:19.212846 Training model: RandomForestClassifier\n",
      "2022-12-04 18:41:24.227114 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 18:41:24.846447 Logging: RandomForestClassifier\n",
      "2022-12-04 18:41:25.026671 Model RandomForestClassifier has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:41:25.026849 Training model: KNeighborsClassifier\n",
      "2022-12-04 18:41:25.041910 Evalutaing model: KNeighborsClassifier\n",
      "2022-12-04 18:41:57.242015 Logging: KNeighborsClassifier\n",
      "2022-12-04 18:41:57.252245 Model KNeighborsClassifier has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:41:57.252447 Training model: XGBClassifier\n",
      "[18:41:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:42:03.355181 Evalutaing model: XGBClassifier\n",
      "2022-12-04 18:42:03.455823 Logging: XGBClassifier\n",
      "2022-12-04 18:42:03.553861 Model XGBClassifier has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:42:04.012044 Training model based on configuration data: Oversampling\n",
      "2022-12-04 18:42:04.012496 Creating model objects.\n",
      "2022-12-04 18:42:04.012966 Model objects created.\n",
      "2022-12-04 18:42:04.051422 Training model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:42:05.718265 Evalutaing model: LogisticRegression\n",
      "2022-12-04 18:42:05.763827 Logging: LogisticRegression\n",
      "2022-12-04 18:42:05.764271 Model LogisticRegression has been trained for configuration data Oversampling.\n",
      "2022-12-04 18:42:05.764360 Training model: DecisionTreeClassifier\n",
      "2022-12-04 18:42:06.423564 Evalutaing model: DecisionTreeClassifier\n",
      "2022-12-04 18:42:06.473158 Logging: DecisionTreeClassifier\n",
      "2022-12-04 18:42:06.476167 Model DecisionTreeClassifier has been trained for configuration data Oversampling.\n",
      "2022-12-04 18:42:06.476310 Training model: RandomForestClassifier\n",
      "2022-12-04 18:42:24.948625 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 18:42:25.632340 Logging: RandomForestClassifier\n",
      "2022-12-04 18:42:25.970139 Model RandomForestClassifier has been trained for configuration data Oversampling.\n",
      "2022-12-04 18:42:25.970349 Training model: KNeighborsClassifier\n",
      "2022-12-04 18:42:26.007453 Evalutaing model: KNeighborsClassifier\n",
      "2022-12-04 18:44:10.129143 Logging: KNeighborsClassifier\n",
      "2022-12-04 18:44:10.135788 Model KNeighborsClassifier has been trained for configuration data Oversampling.\n",
      "2022-12-04 18:44:10.136029 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2022-12-04 18:44:27.463857 Evalutaing model: XGBClassifier\n",
      "2022-12-04 18:44:27.559975 Logging: XGBClassifier\n",
      "2022-12-04 18:44:27.662082 Model XGBClassifier has been trained for configuration data Oversampling.\n",
      "2022-12-04 18:44:28.410008 Training model based on configuration data: SMOTE\n",
      "2022-12-04 18:44:28.410199 Creating model objects.\n",
      "2022-12-04 18:44:28.410349 Model objects created.\n",
      "2022-12-04 18:44:28.479093 Training model: LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:44:30.133650 Evalutaing model: LogisticRegression\n",
      "2022-12-04 18:44:30.179137 Logging: LogisticRegression\n",
      "2022-12-04 18:44:30.179529 Model LogisticRegression has been trained for configuration data SMOTE.\n",
      "2022-12-04 18:44:30.179603 Training model: DecisionTreeClassifier\n",
      "2022-12-04 18:44:30.807656 Evalutaing model: DecisionTreeClassifier\n",
      "2022-12-04 18:44:30.852499 Logging: DecisionTreeClassifier\n",
      "2022-12-04 18:44:30.855321 Model DecisionTreeClassifier has been trained for configuration data SMOTE.\n",
      "2022-12-04 18:44:30.855465 Training model: RandomForestClassifier\n",
      "2022-12-04 18:44:49.846467 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 18:44:50.569542 Logging: RandomForestClassifier\n",
      "2022-12-04 18:44:50.892236 Model RandomForestClassifier has been trained for configuration data SMOTE.\n",
      "2022-12-04 18:44:50.892445 Training model: KNeighborsClassifier\n",
      "2022-12-04 18:44:50.930809 Evalutaing model: KNeighborsClassifier\n",
      "2022-12-04 18:46:38.472233 Logging: KNeighborsClassifier\n",
      "2022-12-04 18:46:38.479131 Model KNeighborsClassifier has been trained for configuration data SMOTE.\n",
      "2022-12-04 18:46:38.479298 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2022-12-04 18:46:59.183886 Evalutaing model: XGBClassifier\n",
      "2022-12-04 18:46:59.280943 Logging: XGBClassifier\n",
      "2022-12-04 18:46:59.398480 Model XGBClassifier has been trained for configuration data SMOTE.\n",
      "2022-12-04 18:47:00.186671 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "list_of_trained_model, training_log = train_eval(\"Baseline\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Choose Best Performance Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_model(list_of_model, training_log, params):\n",
    "    # Create copy list of model\n",
    "    list_of_model = copy.deepcopy(list_of_model)\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Choosing model by metrics score.\")\n",
    "\n",
    "    # Create required predefined variabel\n",
    "    curr_production_model = None\n",
    "    prev_production_model = None\n",
    "    production_model_log = None\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Converting training log type of data from dict to dataframe.\")\n",
    "\n",
    "    # Convert dictionary to pandas for easy operation\n",
    "    training_log = pd.DataFrame(copy.deepcopy(training_log))\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Trying to load previous production model.\")\n",
    "\n",
    "    # Check if there is a previous production model\n",
    "    try:\n",
    "        prev_production_model = util.pickle_load(params[\"production_model_path\"])\n",
    "        util.print_debug(\"Previous production model loaded.\")\n",
    "\n",
    "    except FileNotFoundError as fe:\n",
    "        util.print_debug(\"No previous production model detected, choosing best model only from current trained model.\")\n",
    "\n",
    "    # If previous production model detected:\n",
    "    if prev_production_model != None:\n",
    "        # Debug message\n",
    "        util.print_debug(\"Loading validation data.\")\n",
    "        x_valid, y_valid = load_valid_feng(params)\n",
    "        \n",
    "        # Debug message\n",
    "        util.print_debug(\"Checking compatibilty previous production model's input with current train data's features.\")\n",
    "\n",
    "        # Check list features of previous production model and current dataset\n",
    "        production_model_features = set(prev_production_model[\"model_data\"][\"model_object\"].feature_names_in_)\n",
    "        current_dataset_features = set(x_valid.columns)\n",
    "        number_of_different_features = len((production_model_features - current_dataset_features) | (current_dataset_features - production_model_features))\n",
    "\n",
    "        # If feature matched:\n",
    "        if number_of_different_features == 0:\n",
    "            # Debug message\n",
    "            util.print_debug(\"Features compatible.\")\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Reassesing previous model performance using current validation data.\")\n",
    "\n",
    "            # Re-predict previous production model to provide valid metrics compared to other current models\n",
    "            y_pred = prev_production_model[\"model_data\"][\"model_object\"].predict(x_valid)\n",
    "\n",
    "            # Re-asses prediction result\n",
    "            eval_res = classification_report(y_valid, y_pred, output_dict = True)\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Assessing complete.\")\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Storing new metrics data to previous model structure.\")\n",
    "\n",
    "            # Update their performance log\n",
    "            prev_production_model[\"model_log\"][\"performance\"] = eval_res\n",
    "            prev_production_model[\"model_log\"][\"f1_score_avg\"] = eval_res[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Adding previous model data to current training log and list of model\")\n",
    "\n",
    "            # Added previous production model log to current logs to compere who has the greatest f1 score\n",
    "            training_log = pd.concat([training_log, pd.DataFrame([prev_production_model[\"model_log\"]])])\n",
    "\n",
    "            # Added previous production model to current list of models to choose from if it has the greatest f1 score\n",
    "            list_of_model[\"prev_production_model\"] = [copy.deepcopy(prev_production_model[\"model_data\"])]\n",
    "        else:\n",
    "            # To indicate that we are not using previous production model\n",
    "            prev_production_model = None\n",
    "\n",
    "            # Debug message\n",
    "            util.print_debug(\"Different features between production model with current dataset is detected, ignoring production dataset.\")\n",
    "\n",
    "    # Debug message\n",
    "    util.print_debug(\"Sorting training log by f1 macro avg and training time.\")\n",
    "\n",
    "    # Sort training log by f1 score macro avg and trining time\n",
    "    best_model_log = training_log.sort_values([\"f1_score_avg\", \"training_time\"], ascending = [False, True]).iloc[0]\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Searching model data based on sorted training log.\")\n",
    "\n",
    "    # Get model object with greatest f1 score macro avg by using UID\n",
    "    for configuration_data in list_of_model:\n",
    "        for model_data in list_of_model[configuration_data]:\n",
    "            if model_data[\"model_uid\"] == best_model_log[\"model_uid\"]:\n",
    "                curr_production_model = dict()\n",
    "                curr_production_model[\"model_data\"] = copy.deepcopy(model_data)\n",
    "                curr_production_model[\"model_log\"] = copy.deepcopy(best_model_log.to_dict())\n",
    "                curr_production_model[\"model_log\"][\"model_name\"] = \"Production-{}\".format(curr_production_model[\"model_data\"][\"model_name\"])\n",
    "                curr_production_model[\"model_log\"][\"training_date\"] = str(curr_production_model[\"model_log\"][\"training_date\"])\n",
    "                production_model_log = training_log_updater(curr_production_model[\"model_log\"], params)\n",
    "                break\n",
    "    \n",
    "    # In case UID not found\n",
    "    if curr_production_model == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    \n",
    "    # Debug message\n",
    "    util.print_debug(\"Model chosen.\")\n",
    "\n",
    "    # Dump chosen production model\n",
    "    util.pickle_dump(curr_production_model, params[\"production_model_path\"])\n",
    "    \n",
    "    # Return current chosen production model, log of production models and current training log\n",
    "    return curr_production_model, production_model_log, training_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:49:34.399020 Choosing model by metrics score.\n",
      "2022-12-04 18:49:34.399228 Converting training log type of data from dict to dataframe.\n",
      "2022-12-04 18:49:34.412179 Trying to load previous production model.\n",
      "2022-12-04 18:49:36.421775 Previous production model loaded.\n",
      "2022-12-04 18:49:36.421961 Loading validation data.\n",
      "2022-12-04 18:49:36.451626 Checking compatibilty previous production model's input with current train data's features.\n",
      "2022-12-04 18:49:36.524132 Different features between production model with current dataset is detected, ignoring production dataset.\n",
      "2022-12-04 18:49:36.524475 Sorting training log by f1 macro avg and training time.\n",
      "2022-12-04 18:49:36.532408 Searching model data based on sorted training log.\n",
      "2022-12-04 18:49:37.126682 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_params(model_name: str) -> dict:\n",
    "    # Define models paramteres\n",
    "    dist_params_xgb = {\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500]\n",
    "    }\n",
    "    dist_params_dct = {\n",
    "        \"algorithm\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_sample_split\" : [1, 2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_sample_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "    dist_params_knn = {\n",
    "        \"creterion\" : [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        \"n_neighbors\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "        \"leaf_size\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "    }\n",
    "    dist_params_lgr = {\n",
    "        \"penalty\" : [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "        \"C\" : [0.01, 0.05, 0.10, 0.15, 0.20, 0.30, 0.60, 0.90, 1],\n",
    "        \"solver\" : [\"saga\"],\n",
    "        \"max_iter\" : [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "    }\n",
    "    dist_params_rfc = {\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500],\n",
    "        \"min_samples_split\" : [1, 2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_samples_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "\n",
    "    # Make all models parameters in to one\n",
    "    dist_params = {\n",
    "        \"XGBClassifier\": dist_params_xgb,\n",
    "        \"DecisionTreeClassifier\": dist_params_dct,\n",
    "        \"KNeighborsClassifier\": dist_params_knn,\n",
    "        \"LogisticRegression\": dist_params_lgr,\n",
    "        \"RandomForestClassifier\": dist_params_rfc\n",
    "    }\n",
    "\n",
    "    # Return distribution of model parameters\n",
    "    return dist_params[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_params_tuning(model: dict) -> list:\n",
    "    # Create copy of current best baseline model\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    # Create model's parameter distribution\n",
    "    dist_params = create_dist_params(model[\"model_data\"][\"model_name\"])\n",
    "\n",
    "    # Create model object\n",
    "    model_rsc = RandomizedSearchCV(model[\"model_data\"][\"model_object\"], dist_params, n_jobs = -1)\n",
    "    model_data = {\n",
    "        \"model_name\": model[\"model_data\"][\"model_name\"],\n",
    "        \"model_object\": model_rsc,\n",
    "        \"model_uid\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Return model object\n",
    "    return [model_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:49:45.757486 Loading dataset.\n",
      "2022-12-04 18:49:46.189495 Dataset loaded.\n",
      "2022-12-04 18:49:46.189696 Creating training log template.\n",
      "2022-12-04 18:49:46.189792 Training log template created.\n",
      "2022-12-04 18:49:46.189882 Training model based on configuration data: Undersampling\n",
      "2022-12-04 18:49:46.580992 Training model: RandomForestClassifier\n",
      "2022-12-04 18:52:21.430295 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 18:52:22.314195 Logging: RandomForestClassifier\n",
      "2022-12-04 18:52:22.759070 Model RandomForestClassifier has been trained for configuration data Undersampling.\n",
      "2022-12-04 18:52:23.191032 Training model based on configuration data: Oversampling\n",
      "2022-12-04 18:52:24.012525 Training model: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.68311146 0.68778289 0.71746339 0.68337771        nan 0.6960547\n",
      "        nan 0.68320828        nan 0.70101658]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:02:49.736305 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 19:02:51.393093 Logging: RandomForestClassifier\n",
      "2022-12-04 19:02:53.371871 Model RandomForestClassifier has been trained for configuration data Oversampling.\n",
      "2022-12-04 19:02:55.636169 Training model based on configuration data: SMOTE\n",
      "2022-12-04 19:02:56.382133 Training model: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 476, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 189, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.69197628        nan 0.69543144 0.69516519 0.69924362 0.69103231\n",
      "        nan        nan 0.69538908 0.6903788 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:11:06.945326 Evalutaing model: RandomForestClassifier\n",
      "2022-12-04 19:11:07.190484 Logging: RandomForestClassifier\n",
      "2022-12-04 19:11:07.552916 Model RandomForestClassifier has been trained for configuration data SMOTE.\n",
      "2022-12-04 19:11:07.910701 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "list_of_trained_model, training_log = train_eval(\"Hyperparams_Tuning\", params, hyper_params_tuning(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-04 19:21:28.110662 Choosing model by metrics score.\n",
      "2022-12-04 19:21:28.110953 Converting training log type of data from dict to dataframe.\n",
      "2022-12-04 19:21:28.115647 Trying to load previous production model.\n",
      "2022-12-04 19:21:28.990278 Previous production model loaded.\n",
      "2022-12-04 19:21:28.990475 Loading validation data.\n",
      "2022-12-04 19:21:29.020333 Checking compatibilty previous production model's input with current train data's features.\n",
      "2022-12-04 19:21:29.020558 Features compatible.\n",
      "2022-12-04 19:21:29.020623 Reassesing previous model performance using current validation data.\n",
      "2022-12-04 19:21:29.677917 Assessing complete.\n",
      "2022-12-04 19:21:29.678086 Storing new metrics data to previous model structure.\n",
      "2022-12-04 19:21:29.678141 Adding previous model data to current training log and list of model\n",
      "2022-12-04 19:21:30.027298 Sorting training log by f1 macro avg and training time.\n",
      "2022-12-04 19:21:30.031320 Searching model data based on sorted training log.\n",
      "2022-12-04 19:21:30.653626 Model chosen.\n"
     ]
    }
   ],
   "source": [
    "model, production_model_log, training_logs = get_production_model(list_of_trained_model, training_log, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Take a Look at Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, y_valid = load_valid_feng(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model[\"model_data\"][\"model_object\"].predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f9cc2005700>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 3636\n",
    "TN = 11089\n",
    "FP = 6617\n",
    "FN = 2077\n",
    "\n",
    "ACC = (TN+TP)/(TN+FP+FN+TP)\n",
    "P = TP/(TP+FP)\n",
    "R = TP/(TP+FN)\n",
    "F1 = 2*(P*R/(P+R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6287629702378411\n",
      "0.3546279137813323\n",
      "0.636443199719937\n",
      "0.455467869222097\n"
     ]
    }
   ],
   "source": [
    "print(ACC)\n",
    "print(P)\n",
    "print(R)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6b5ba345f865ccab9c1d1dba86e394ca61d67366183ce7e182371ee31a721f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
